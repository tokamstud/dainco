{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1) Open and inspect the file **\n",
    "#### We open hadoop_1m.txt file in which we know from earlier work on MapReduce\n",
    "#### We also inspect its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "[u'From general-return-2133-apmail-hadoop-general-archive=hadoop.apache.org@hadoop.apache.org Fri Oct 01 15:07:28 2010', u'Return-Path: <general-return-2133-apmail-hadoop-general-archive=hadoop.apache.org@hadoop.apache.org>', u'Delivered-To: apmail-hadoop-general-archive@minotaur.apache.org', u'Received: (qmail 98655 invoked from network); 1 Oct 2010 15:07:28 -0000', u'Received: from unknown (HELO mail.apache.org) (140.211.11.3)']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('hadoop_1m.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "hadoop_rdd = (sc\n",
    "              .textFile(fileName))\n",
    "\n",
    "print hadoop_rdd.count()\n",
    "print hadoop_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### ** (2) Count email domains **\n",
    "#### Compare to Hadoop MapReduce code from earlier lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "[(u'nokia.com', 2), (u'sabalcore.com', 3), (u'cs.ucf.ed', 6), (u'cloudera.com', 442), (u'schiessle.org', 5)]\n"
     ]
    }
   ],
   "source": [
    "domain_counts = (hadoop_rdd\n",
    "                 .filter(lambda line: line.find(\"From:\") == 0)\n",
    "                 .map(lambda line: (line[line.find(\"@\")+1:line.find(\">\")],1))\n",
    "                 .reduceByKey(lambda a, b: a+b))\n",
    "\n",
    "print domain_counts.count()\n",
    "print domain_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comapre reduceByKey to reduce\n",
    "#### domain_counts above finishes with reduce, we see below that the result of such operation is an RDD, so it's a transformation\n",
    "#### If we run reduce on domain_counts it becomes just a number which is returned to the driver, so it's an action, like take or count above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[63] at RDD at PythonRDD.scala:43\n",
      "11424\n"
     ]
    }
   ],
   "source": [
    "print domain_counts\n",
    "\n",
    "print domain_counts.map(lambda (a, b): b).reduce(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Spark SQL\n",
    "####Load library and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('hadoop_1m.csv')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "hadoop_csv_rdd = (sc\n",
    "                  .textFile(fileName))\n",
    "\n",
    "emails = (hadoop_csv_rdd\n",
    "         .map(lambda l: l.split(\",\"))\n",
    "         .map(lambda p: Row(id=p[0], list=p[1], date1=p[2], date2=p[3], email=p[4], subject=p[5])))\n",
    "\n",
    "schemaEmails = sqlContext.createDataFrame(emails)\n",
    "schemaEmails.registerTempTable(\"emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date1=u'\"Fri', date2=u' 1 Oct 2010 12:06:54 -0300\"', email=u'marcoscba@gmail.com', id=u'AANLkTin6ZUrWYUscAXmq0bOG8mWA35xcAfvLD-jYVzwu@mail.gmail.com', list=u' general.hadoop.apache.org', subject=u'\"Is it possible write directly to datanode\\'s?\"'),\n",
       " Row(date1=u'\"Fri', date2=u' 1 Oct 2010 21:27:22 -0700\"', email=u'rmalviya@apple.com', id=u'1C4C43FE-05F2-464B-9F04-AC972988C326@apple.com', list=u' general.hadoop.apache.org', subject=u'\"Total Space Available on Hadoop Cluster Or Hadoop version of \"df\".\"'),\n",
       " Row(date1=u'\"Sat', date2=u' 2 Oct 2010 16:13:18 +1000\"', email=u'Glenn.Gore@melbourneit.com.au', id=u'C586A8404F8B304E807B4B8BF73B376EA600AC@WIC001MITEBCLV1.messaging.mit', list=u' general.hadoop.apache.org', subject=u'\"RE: Total Space Available on Hadoop Cluster Or Hadoop version of \"df\".\"'),\n",
       " Row(date1=u'\"Sat', date2=u' 2 Oct 2010 08:12:48 -0400\"', email=u'palmercliff@gmail.com', id=u'AANLkTimtEM4yzQSC5F16j4r0eqd0MeDb+7FF3FT1v3Yk@mail.gmail.com', list=u' general.hadoop.apache.org', subject=u'\"Re: Is it possible write directly to datanode\\'s?\"'),\n",
       " Row(date1=u'\"Sat', date2=u' 2 Oct 2010 10:39:09 -0400\"', email=u'marcoscba@gmail.com', id=u'AANLkTikT3H2y_EJ-tVgPvYjSVN7NouHvFTXXYR5vi044@mail.gmail.com', list=u' general.hadoop.apache.org', subject=u'\"Re: Total Space Available on Hadoop Cluster Or Hadoop version of \"df\".\"')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c0=627)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = sqlContext.sql(\"SELECT COUNT(DISTINCT(email)) FROM emails\")\n",
    "lists.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case of problems check if every line has enough fields\n",
    "#### Spark SQL is more sensitive to missing data than Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sc.textFile(\"./data/hadoop_1m.csv\")\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "    .filter(lambda line: len(line)<6)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original query from 101 should work from Spark 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = sqlContext.sql(\"SELECT substr(email, locate('@', email)+1), count(substr(email,locate('@', email)+1)) FROM emails GROUP BY substr(email,locate('@', email)+1)\")\n",
    "results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
